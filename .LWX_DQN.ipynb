{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238253a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import torch\n",
    "from envs.aquaculture_env import AquacultureEnv\n",
    "from utils.plot_callback import PlotCallback\n",
    "\n",
    "\n",
    "# Discretized environment to handle continuous action space\n",
    "class DiscretizedAquacultureEnv(Env):\n",
    "    def __init__(self, region=\"guangdong\"):\n",
    "        self.base_env = AquacultureEnv(region=region)\n",
    "        \n",
    "        # Discretize the action space\n",
    "        self.feed_bins = 40\n",
    "        self.temp_bins = 16\n",
    "        self.air_bins = 10\n",
    "\n",
    "        self.discrete_actions = [\n",
    "            (feed, temp, air)\n",
    "            for feed in np.linspace(self.base_env.action_space.low[0], self.base_env.action_space.high[0], self.feed_bins)\n",
    "            for temp in np.linspace(self.base_env.action_space.low[1], self.base_env.action_space.high[1], self.temp_bins)\n",
    "            for air in np.linspace(self.base_env.action_space.low[2], self.base_env.action_space.high[2], self.air_bins)\n",
    "        ]\n",
    "        \n",
    "        self.action_space = Discrete(len(self.discrete_actions))\n",
    "        self.observation_space = self.base_env.observation_space\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.base_env.reset(**kwargs)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action_idx):\n",
    "        action = np.array(self.discrete_actions[action_idx], dtype=np.float32)\n",
    "        obs, reward, terminated, truncated, info = self.base_env.step(action)\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.base_env.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.base_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "536d426e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\n",
      "ðŸš€ Training with exploration_final_eps = 0.05\n",
      "Logging to ./aqua_tensorboard_dqn_eps_0.05\\DQN_4\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | -18.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 419      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 720      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.252    |\n",
      "|    n_updates        | 154      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | -13.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 423      |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 1440     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.174    |\n",
      "|    n_updates        | 334      |\n",
      "----------------------------------\n",
      "âœ… Training curve saved to: training_rewards_eps_0.05.png\n",
      "ðŸ“Š Total reward: -153.58\n",
      "ðŸ“‰ Reward variation (std dev): 23.33\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\n",
      "ðŸš€ Training with exploration_final_eps = 0.01\n",
      "Logging to ./aqua_tensorboard_dqn_eps_0.01\\DQN_1\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 8.52     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 474      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 720      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0559   |\n",
      "|    n_updates        | 154      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 12.3     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 450      |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 1440     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0227   |\n",
      "|    n_updates        | 334      |\n",
      "----------------------------------\n",
      "âœ… Training curve saved to: training_rewards_eps_0.01.png\n",
      "ðŸ“Š Total reward: 133.18\n",
      "ðŸ“‰ Reward variation (std dev): 8.23\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\n",
      "ðŸš€ Training with exploration_final_eps = 0.001\n",
      "Logging to ./aqua_tensorboard_dqn_eps_0.001\\DQN_1\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 4.01     |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 471      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 720      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0664   |\n",
      "|    n_updates        | 154      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 41.8     |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 442      |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 1440     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.14     |\n",
      "|    n_updates        | 334      |\n",
      "----------------------------------\n",
      "âœ… Training curve saved to: training_rewards_eps_0.001.png\n",
      "ðŸ“Š Total reward: 531.74\n",
      "ðŸ“‰ Reward variation (std dev): 48.77\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available for model training\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Create the environment and validate it\n",
    "env = DiscretizedAquacultureEnv(region=\"north_sulawesi\")\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# Initialize the DQN model\n",
    "for eps in [0.05, 0.01, 0.001]:\n",
    "    plot_cb = PlotCallback(\n",
    "        window=10,\n",
    "        save_path=f\"training_rewards_eps_{eps}.png\",\n",
    "        title=f\"Training Rewards (Final Îµ={eps})\"\n",
    "    )\n",
    "\n",
    "    model = DQN(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        verbose=1,\n",
    "        tensorboard_log=f\"./aqua_tensorboard_dqn_eps_{eps}\",\n",
    "        learning_rate=1e-4,\n",
    "        batch_size=256,\n",
    "        gamma=0.99,\n",
    "        exploration_final_eps=eps,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "    print(f\"\\nðŸš€ Training with exploration_final_eps = {eps}\")\n",
    "    model.learn(total_timesteps=180 * 10, callback=plot_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "041bbd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 17:29:10,986] A new study created in memory with name: no-name-d305c081-5c76-40a2-a3bb-2cc59fc639f8\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [64, 64] which is of type list.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [128, 128] which is of type list.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=900, episode_reward=0.10 +/- 0.97\n",
      "Episode length: 180.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1800, episode_reward=0.01 +/- 0.99\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=2700, episode_reward=5.53 +/- 2.14\n",
      "Episode length: 180.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3600, episode_reward=58.48 +/- 1.45\n",
      "Episode length: 180.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4500, episode_reward=101.09 +/- 2.08\n",
      "Episode length: 180.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5400, episode_reward=101.60 +/- 1.54\n",
      "Episode length: 180.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6300, episode_reward=101.10 +/- 1.41\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=7200, episode_reward=55.85 +/- 2.27\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=8100, episode_reward=10.95 +/- 3.02\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-14.89 +/- 3.98\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9900, episode_reward=37.59 +/- 3.31\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=10800, episode_reward=158.09 +/- 0.66\n",
      "Episode length: 180.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11700, episode_reward=-51.78 +/- 3.54\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=12600, episode_reward=62.01 +/- 1.65\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=13500, episode_reward=125.59 +/- 1.45\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=14400, episode_reward=124.80 +/- 1.27\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=15300, episode_reward=167.26 +/- 0.71\n",
      "Episode length: 180.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16200, episode_reward=180.82 +/- 0.55\n",
      "Episode length: 180.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=17100, episode_reward=144.45 +/- 2.24\n",
      "Episode length: 180.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 17:30:31,746] Trial 0 finished with value: 180.81518916226923 and parameters: {'learning_rate': 0.00010290358907837275, 'gamma': 0.9826490514542094, 'batch_size': 32, 'buffer_size': 1000000, 'target_update_interval': 5000, 'net_arch': [128, 128]}. Best is trial 0 with value: 180.81518916226923.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=18000, episode_reward=38.78 +/- 3.93\n",
      "Episode length: 180.00 +/- 0.00\n",
      "âœ… Training curve saved to: plots\\20250425_172910_trial00_rewards.png\n",
      "ðŸ“Š Total reward: 7957.21\n",
      "ðŸ“‰ Reward variation (std dev): 61.83\n",
      "Eval num_timesteps=900, episode_reward=-105.45 +/- 2.48\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=1800, episode_reward=24.47 +/- 1.47\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=2700, episode_reward=32.06 +/- 1.31\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=3600, episode_reward=-83.21 +/- 1.93\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=20.98 +/- 1.66\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=5400, episode_reward=-195.85 +/- 2.64\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=6300, episode_reward=15.62 +/- 1.92\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=7200, episode_reward=-21.02 +/- 3.24\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=8100, episode_reward=-1.31 +/- 1.99\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=24.54 +/- 2.98\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9900, episode_reward=26.05 +/- 5.58\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=10800, episode_reward=160.81 +/- 1.36\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=11700, episode_reward=127.55 +/- 1.22\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=12600, episode_reward=118.48 +/- 3.67\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=13500, episode_reward=144.93 +/- 0.85\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=14400, episode_reward=87.57 +/- 1.65\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=15300, episode_reward=-34.16 +/- 2.56\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=16200, episode_reward=106.67 +/- 1.06\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=17100, episode_reward=94.53 +/- 1.55\n",
      "Episode length: 180.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 17:31:55,459] Trial 1 finished with value: 180.81518916226923 and parameters: {'learning_rate': 0.00013078368361118422, 'gamma': 0.9871982329503499, 'batch_size': 64, 'buffer_size': 500000, 'target_update_interval': 10000, 'net_arch': [128, 128]}. Best is trial 0 with value: 180.81518916226923.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=18000, episode_reward=119.74 +/- 1.05\n",
      "Episode length: 180.00 +/- 0.00\n",
      "âœ… Training curve saved to: plots\\20250425_172910_trial01_rewards.png\n",
      "ðŸ“Š Total reward: 2639.53\n",
      "ðŸ“‰ Reward variation (std dev): 76.50\n",
      "Eval num_timesteps=900, episode_reward=47.53 +/- 1.46\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=1800, episode_reward=28.36 +/- 2.40\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=2700, episode_reward=29.51 +/- 2.15\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=3600, episode_reward=9.98 +/- 2.53\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=-73.76 +/- 5.67\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=5400, episode_reward=80.44 +/- 2.11\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=6300, episode_reward=32.54 +/- 5.20\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=7200, episode_reward=-35.65 +/- 2.04\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=8100, episode_reward=-58.13 +/- 3.32\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=158.22 +/- 1.36\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9900, episode_reward=152.36 +/- 2.08\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=10800, episode_reward=135.44 +/- 1.29\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=11700, episode_reward=186.31 +/- 0.68\n",
      "Episode length: 180.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12600, episode_reward=151.13 +/- 0.87\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=13500, episode_reward=181.51 +/- 0.75\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=14400, episode_reward=99.67 +/- 10.95\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=15300, episode_reward=125.75 +/- 2.04\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=16200, episode_reward=54.96 +/- 2.63\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=17100, episode_reward=101.95 +/- 2.51\n",
      "Episode length: 180.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 17:33:18,729] Trial 2 finished with value: 186.3059869751334 and parameters: {'learning_rate': 0.00011556060083557823, 'gamma': 0.9949461829904909, 'batch_size': 64, 'buffer_size': 1000000, 'target_update_interval': 5000, 'net_arch': [128, 128]}. Best is trial 2 with value: 186.3059869751334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=18000, episode_reward=171.06 +/- 0.78\n",
      "Episode length: 180.00 +/- 0.00\n",
      "âœ… Training curve saved to: plots\\20250425_172910_trial02_rewards.png\n",
      "ðŸ“Š Total reward: 8020.43\n",
      "ðŸ“‰ Reward variation (std dev): 77.06\n",
      "Eval num_timesteps=900, episode_reward=36.86 +/- 0.19\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=1800, episode_reward=29.23 +/- 2.49\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=2700, episode_reward=-113.69 +/- 3.00\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=3600, episode_reward=-184.11 +/- 10.90\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=17.18 +/- 1.96\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=5400, episode_reward=-39.32 +/- 3.44\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=6300, episode_reward=-89.28 +/- 3.81\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=7200, episode_reward=-126.04 +/- 6.38\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=8100, episode_reward=-84.02 +/- 27.47\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=15.38 +/- 1.69\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9900, episode_reward=-233.98 +/- 4.25\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=10800, episode_reward=99.53 +/- 1.39\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=11700, episode_reward=128.21 +/- 1.55\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=12600, episode_reward=51.78 +/- 2.03\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=13500, episode_reward=56.88 +/- 7.03\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=14400, episode_reward=80.67 +/- 1.74\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=15300, episode_reward=63.08 +/- 3.87\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=16200, episode_reward=105.14 +/- 1.37\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=17100, episode_reward=73.75 +/- 2.27\n",
      "Episode length: 180.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 17:34:30,722] Trial 3 finished with value: 186.3059869751334 and parameters: {'learning_rate': 0.00033430456481121045, 'gamma': 0.9917043138355404, 'batch_size': 32, 'buffer_size': 500000, 'target_update_interval': 10000, 'net_arch': [64, 64]}. Best is trial 2 with value: 186.3059869751334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=18000, episode_reward=118.07 +/- 1.02\n",
      "Episode length: 180.00 +/- 0.00\n",
      "âœ… Training curve saved to: plots\\20250425_172910_trial03_rewards.png\n",
      "ðŸ“Š Total reward: 298.16\n",
      "ðŸ“‰ Reward variation (std dev): 70.48\n",
      "Eval num_timesteps=900, episode_reward=-139.57 +/- 3.35\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=1800, episode_reward=-52.15 +/- 3.41\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=2700, episode_reward=-16.07 +/- 2.42\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=3600, episode_reward=0.80 +/- 2.42\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=-32.13 +/- 2.39\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=5400, episode_reward=20.29 +/- 1.43\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=6300, episode_reward=10.56 +/- 1.28\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=7200, episode_reward=-236.89 +/- 3.01\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=8100, episode_reward=51.13 +/- 2.34\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=63.12 +/- 2.85\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9900, episode_reward=-182.16 +/- 4.37\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=10800, episode_reward=46.35 +/- 1.36\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=11700, episode_reward=64.54 +/- 1.96\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=12600, episode_reward=31.94 +/- 1.90\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=13500, episode_reward=-74.40 +/- 3.19\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=14400, episode_reward=-5.09 +/- 3.60\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=15300, episode_reward=-124.03 +/- 2.66\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=16200, episode_reward=-28.80 +/- 2.07\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=17100, episode_reward=-52.45 +/- 2.51\n",
      "Episode length: 180.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 17:35:43,309] Trial 4 finished with value: 186.3059869751334 and parameters: {'learning_rate': 0.00018404149583961342, 'gamma': 0.9859821217639309, 'batch_size': 32, 'buffer_size': 500000, 'target_update_interval': 10000, 'net_arch': [64, 64]}. Best is trial 2 with value: 186.3059869751334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=18000, episode_reward=-77.78 +/- 6.69\n",
      "Episode length: 180.00 +/- 0.00\n",
      "âœ… Training curve saved to: plots\\20250425_172910_trial04_rewards.png\n",
      "ðŸ“Š Total reward: -873.56\n",
      "ðŸ“‰ Reward variation (std dev): 61.85\n",
      "Eval num_timesteps=900, episode_reward=-18.31 +/- 2.35\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=1800, episode_reward=108.82 +/- 1.37\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=2700, episode_reward=57.52 +/- 4.68\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=3600, episode_reward=-75.13 +/- 6.99\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=31.89 +/- 4.19\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=5400, episode_reward=-1.27 +/- 4.50\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=6300, episode_reward=-56.97 +/- 28.55\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=7200, episode_reward=-1.08 +/- 1.96\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=8100, episode_reward=-15.63 +/- 1.94\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-60.73 +/- 7.80\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9900, episode_reward=-82.40 +/- 8.85\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=10800, episode_reward=159.09 +/- 0.91\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=11700, episode_reward=137.29 +/- 1.89\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=12600, episode_reward=-3.65 +/- 2.37\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=13500, episode_reward=15.75 +/- 2.20\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=14400, episode_reward=40.40 +/- 1.42\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=15300, episode_reward=149.66 +/- 3.05\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=16200, episode_reward=92.90 +/- 21.00\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=17100, episode_reward=167.50 +/- 0.94\n",
      "Episode length: 180.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 17:36:55,220] Trial 5 finished with value: 186.3059869751334 and parameters: {'learning_rate': 0.0004260295172958155, 'gamma': 0.9868218818271846, 'batch_size': 32, 'buffer_size': 500000, 'target_update_interval': 10000, 'net_arch': [64, 64]}. Best is trial 2 with value: 186.3059869751334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=18000, episode_reward=174.93 +/- 0.80\n",
      "Episode length: 180.00 +/- 0.00\n",
      "âœ… Training curve saved to: plots\\20250425_172910_trial05_rewards.png\n",
      "ðŸ“Š Total reward: 4352.11\n",
      "ðŸ“‰ Reward variation (std dev): 68.38\n",
      "Eval num_timesteps=900, episode_reward=7.83 +/- 2.48\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=1800, episode_reward=98.05 +/- 1.18\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=2700, episode_reward=5.42 +/- 2.58\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=3600, episode_reward=100.98 +/- 1.36\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=25.03 +/- 9.65\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=5400, episode_reward=-28.45 +/- 4.22\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=6300, episode_reward=3.25 +/- 2.50\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=7200, episode_reward=107.93 +/- 2.04\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=8100, episode_reward=-30.92 +/- 3.88\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-10.61 +/- 2.24\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9900, episode_reward=44.21 +/- 4.46\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=10800, episode_reward=152.28 +/- 1.08\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=11700, episode_reward=45.18 +/- 2.39\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=12600, episode_reward=67.95 +/- 1.69\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=13500, episode_reward=73.20 +/- 2.79\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=14400, episode_reward=115.56 +/- 1.39\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=15300, episode_reward=-7.97 +/- 6.97\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=16200, episode_reward=11.21 +/- 5.68\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=17100, episode_reward=97.53 +/- 23.73\n",
      "Episode length: 180.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 17:38:19,948] Trial 6 finished with value: 186.3059869751334 and parameters: {'learning_rate': 0.00014076983870370925, 'gamma': 0.9924411770042789, 'batch_size': 64, 'buffer_size': 1000000, 'target_update_interval': 10000, 'net_arch': [128, 128]}. Best is trial 2 with value: 186.3059869751334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=18000, episode_reward=145.88 +/- 1.11\n",
      "Episode length: 180.00 +/- 0.00\n",
      "âœ… Training curve saved to: plots\\20250425_172910_trial06_rewards.png\n",
      "ðŸ“Š Total reward: 5083.13\n",
      "ðŸ“‰ Reward variation (std dev): 54.22\n",
      "Eval num_timesteps=900, episode_reward=18.49 +/- 0.58\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=1800, episode_reward=28.65 +/- 1.84\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=2700, episode_reward=54.36 +/- 0.89\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=3600, episode_reward=108.61 +/- 0.96\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=90.41 +/- 0.67\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=5400, episode_reward=81.73 +/- 2.83\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=6300, episode_reward=89.41 +/- 0.73\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=7200, episode_reward=8.03 +/- 3.04\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=8100, episode_reward=-10.47 +/- 29.14\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-22.04 +/- 5.84\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9900, episode_reward=-48.55 +/- 4.59\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=10800, episode_reward=38.65 +/- 2.27\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=11700, episode_reward=23.50 +/- 7.91\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=12600, episode_reward=124.19 +/- 0.85\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=13500, episode_reward=127.15 +/- 1.19\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=14400, episode_reward=87.25 +/- 1.61\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=15300, episode_reward=70.53 +/- 3.10\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=16200, episode_reward=92.32 +/- 1.31\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=17100, episode_reward=97.08 +/- 1.41\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=71.37 +/- 1.44\n",
      "Episode length: 180.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 17:39:32,763] Trial 7 finished with value: 186.3059869751334 and parameters: {'learning_rate': 0.00017050885266499573, 'gamma': 0.9820074338497919, 'batch_size': 32, 'buffer_size': 1000000, 'target_update_interval': 10000, 'net_arch': [64, 64]}. Best is trial 2 with value: 186.3059869751334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training curve saved to: plots\\20250425_172910_trial07_rewards.png\n",
      "ðŸ“Š Total reward: 5736.90\n",
      "ðŸ“‰ Reward variation (std dev): 50.59\n",
      "Eval num_timesteps=900, episode_reward=-1.80 +/- 1.31\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=1800, episode_reward=13.92 +/- 2.31\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=2700, episode_reward=-122.15 +/- 2.97\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=3600, episode_reward=-91.98 +/- 6.68\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=63.46 +/- 1.53\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=5400, episode_reward=-16.74 +/- 3.64\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=6300, episode_reward=34.68 +/- 3.88\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=7200, episode_reward=32.23 +/- 2.12\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=8100, episode_reward=-65.72 +/- 3.91\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=61.72 +/- 1.79\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9900, episode_reward=40.06 +/- 1.80\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=10800, episode_reward=116.17 +/- 1.20\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=11700, episode_reward=2.63 +/- 2.27\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=12600, episode_reward=121.63 +/- 8.30\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=13500, episode_reward=21.22 +/- 1.15\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=14400, episode_reward=114.07 +/- 1.19\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=15300, episode_reward=125.97 +/- 2.22\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=16200, episode_reward=131.70 +/- 1.78\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=17100, episode_reward=110.64 +/- 1.67\n",
      "Episode length: 180.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 17:40:54,950] Trial 8 finished with value: 186.3059869751334 and parameters: {'learning_rate': 0.00026569943815810506, 'gamma': 0.9944819981776538, 'batch_size': 64, 'buffer_size': 500000, 'target_update_interval': 10000, 'net_arch': [128, 128]}. Best is trial 2 with value: 186.3059869751334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=18000, episode_reward=123.71 +/- 9.48\n",
      "Episode length: 180.00 +/- 0.00\n",
      "âœ… Training curve saved to: plots\\20250425_172910_trial08_rewards.png\n",
      "ðŸ“Š Total reward: 4009.29\n",
      "ðŸ“‰ Reward variation (std dev): 74.51\n",
      "Eval num_timesteps=900, episode_reward=26.43 +/- 1.95\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=1800, episode_reward=57.22 +/- 1.70\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=2700, episode_reward=-59.49 +/- 2.35\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=3600, episode_reward=78.05 +/- 1.40\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=81.13 +/- 3.14\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=5400, episode_reward=55.39 +/- 7.27\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=6300, episode_reward=-136.67 +/- 3.21\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=7200, episode_reward=-27.03 +/- 3.72\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=8100, episode_reward=11.63 +/- 2.39\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=18.54 +/- 5.46\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9900, episode_reward=-39.76 +/- 4.97\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=10800, episode_reward=117.74 +/- 1.37\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=11700, episode_reward=31.21 +/- 2.46\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=12600, episode_reward=40.29 +/- 13.35\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=13500, episode_reward=161.47 +/- 0.98\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=14400, episode_reward=161.26 +/- 1.07\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=15300, episode_reward=162.31 +/- 1.30\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=16200, episode_reward=51.98 +/- 1.38\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=17100, episode_reward=141.98 +/- 1.26\n",
      "Episode length: 180.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 17:42:09,377] Trial 9 finished with value: 186.3059869751334 and parameters: {'learning_rate': 0.0003144203221988339, 'gamma': 0.9874347421668123, 'batch_size': 64, 'buffer_size': 1000000, 'target_update_interval': 10000, 'net_arch': [64, 64]}. Best is trial 2 with value: 186.3059869751334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=18000, episode_reward=142.30 +/- 1.03\n",
      "Episode length: 180.00 +/- 0.00\n",
      "âœ… Training curve saved to: plots\\20250425_172910_trial09_rewards.png\n",
      "ðŸ“Š Total reward: 3856.73\n",
      "ðŸ“‰ Reward variation (std dev): 73.94\n",
      "Eval num_timesteps=900, episode_reward=25.37 +/- 0.84\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=1800, episode_reward=9.88 +/- 2.87\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=2700, episode_reward=103.70 +/- 0.48\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=3600, episode_reward=41.17 +/- 2.20\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=-30.96 +/- 2.31\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=5400, episode_reward=-87.12 +/- 3.72\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=6300, episode_reward=6.76 +/- 2.66\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=7200, episode_reward=73.00 +/- 1.40\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=8100, episode_reward=-54.56 +/- 13.54\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=43.32 +/- 18.44\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9900, episode_reward=16.58 +/- 1.09\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=10800, episode_reward=-183.96 +/- 103.33\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=11700, episode_reward=147.72 +/- 2.29\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=12600, episode_reward=161.06 +/- 1.37\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=13500, episode_reward=173.79 +/- 0.85\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=14400, episode_reward=169.82 +/- 0.89\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=15300, episode_reward=164.33 +/- 1.02\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=16200, episode_reward=165.71 +/- 1.16\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=17100, episode_reward=162.94 +/- 1.23\n",
      "Episode length: 180.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 17:43:33,312] Trial 10 finished with value: 186.3059869751334 and parameters: {'learning_rate': 0.00010837906104857781, 'gamma': 0.9905448964962523, 'batch_size': 64, 'buffer_size': 1000000, 'target_update_interval': 5000, 'net_arch': [128, 128]}. Best is trial 2 with value: 186.3059869751334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=18000, episode_reward=157.28 +/- 0.99\n",
      "Episode length: 180.00 +/- 0.00\n",
      "âœ… Training curve saved to: plots\\20250425_172910_trial10_rewards.png\n",
      "ðŸ“Š Total reward: 5764.39\n",
      "ðŸ“‰ Reward variation (std dev): 90.77\n",
      "Eval num_timesteps=900, episode_reward=-4.29 +/- 1.32\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=1800, episode_reward=3.65 +/- 2.64\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=2700, episode_reward=37.66 +/- 3.10\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=3600, episode_reward=68.56 +/- 2.70\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=7.23 +/- 3.05\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=5400, episode_reward=58.48 +/- 2.06\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=6300, episode_reward=15.76 +/- 3.19\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=7200, episode_reward=-97.98 +/- 3.35\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=8100, episode_reward=109.26 +/- 1.32\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=131.97 +/- 1.31\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9900, episode_reward=100.86 +/- 1.24\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=10800, episode_reward=-6.02 +/- 0.09\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=11700, episode_reward=-3.15 +/- 0.53\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=12600, episode_reward=124.32 +/- 1.46\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=13500, episode_reward=130.62 +/- 1.59\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=14400, episode_reward=174.00 +/- 1.10\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=15300, episode_reward=155.11 +/- 1.00\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=16200, episode_reward=-4.02 +/- 3.07\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=17100, episode_reward=172.33 +/- 0.61\n",
      "Episode length: 180.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 17:44:44,228] Trial 11 finished with value: 186.3059869751334 and parameters: {'learning_rate': 0.00039155081485713983, 'gamma': 0.994868830456089, 'batch_size': 32, 'buffer_size': 500000, 'target_update_interval': 5000, 'net_arch': [64, 64]}. Best is trial 2 with value: 186.3059869751334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=18000, episode_reward=169.92 +/- 1.07\n",
      "Episode length: 180.00 +/- 0.00\n",
      "âœ… Training curve saved to: plots\\20250425_172910_trial11_rewards.png\n",
      "ðŸ“Š Total reward: 7285.41\n",
      "ðŸ“‰ Reward variation (std dev): 75.08\n",
      "Eval num_timesteps=900, episode_reward=-7.05 +/- 1.25\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=1800, episode_reward=-3.37 +/- 3.18\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=2700, episode_reward=40.01 +/- 1.75\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=3600, episode_reward=5.23 +/- 1.84\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=12.87 +/- 4.05\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=5400, episode_reward=6.00 +/- 2.48\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=6300, episode_reward=22.15 +/- 1.70\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=7200, episode_reward=6.02 +/- 2.35\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=8100, episode_reward=121.27 +/- 0.93\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=99.21 +/- 1.28\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9900, episode_reward=164.20 +/- 0.55\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=10800, episode_reward=155.21 +/- 1.35\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=11700, episode_reward=133.50 +/- 0.70\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=12600, episode_reward=113.71 +/- 1.25\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=13500, episode_reward=60.61 +/- 3.46\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=14400, episode_reward=97.70 +/- 1.55\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=15300, episode_reward=37.20 +/- 2.71\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=16200, episode_reward=152.01 +/- 1.17\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=17100, episode_reward=135.55 +/- 1.54\n",
      "Episode length: 180.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 17:46:04,276] Trial 12 finished with value: 186.3059869751334 and parameters: {'learning_rate': 0.00023408236598323798, 'gamma': 0.9907504394842735, 'batch_size': 32, 'buffer_size': 500000, 'target_update_interval': 5000, 'net_arch': [128, 128]}. Best is trial 2 with value: 186.3059869751334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=18000, episode_reward=79.11 +/- 1.71\n",
      "Episode length: 180.00 +/- 0.00\n",
      "âœ… Training curve saved to: plots\\20250425_172910_trial12_rewards.png\n",
      "ðŸ“Š Total reward: 7313.16\n",
      "ðŸ“‰ Reward variation (std dev): 73.59\n",
      "Eval num_timesteps=900, episode_reward=-10.17 +/- 1.70\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=1800, episode_reward=-80.49 +/- 3.41\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=2700, episode_reward=-31.38 +/- 1.75\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=3600, episode_reward=-113.99 +/- 4.58\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=-36.91 +/- 11.15\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=5400, episode_reward=0.45 +/- 3.49\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=6300, episode_reward=-3.32 +/- 2.65\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=7200, episode_reward=38.28 +/- 2.08\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=8100, episode_reward=120.30 +/- 2.09\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=94.35 +/- 22.04\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9900, episode_reward=-134.31 +/- 5.37\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=10800, episode_reward=151.58 +/- 1.85\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=11700, episode_reward=144.47 +/- 1.13\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=12600, episode_reward=152.44 +/- 0.80\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=13500, episode_reward=-36.71 +/- 6.21\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=14400, episode_reward=161.28 +/- 0.98\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=15300, episode_reward=141.40 +/- 1.46\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=16200, episode_reward=142.38 +/- 1.26\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=17100, episode_reward=146.99 +/- 0.86\n",
      "Episode length: 180.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 17:47:18,077] Trial 13 finished with value: 186.3059869751334 and parameters: {'learning_rate': 0.00031720836644187937, 'gamma': 0.9918376190696623, 'batch_size': 64, 'buffer_size': 1000000, 'target_update_interval': 5000, 'net_arch': [64, 64]}. Best is trial 2 with value: 186.3059869751334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=18000, episode_reward=153.18 +/- 1.00\n",
      "Episode length: 180.00 +/- 0.00\n",
      "âœ… Training curve saved to: plots\\20250425_172910_trial13_rewards.png\n",
      "ðŸ“Š Total reward: 6865.00\n",
      "ðŸ“‰ Reward variation (std dev): 77.75\n",
      "Eval num_timesteps=900, episode_reward=-144.94 +/- 2.46\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=1800, episode_reward=-3.38 +/- 2.46\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=2700, episode_reward=-26.39 +/- 2.67\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=3600, episode_reward=-32.63 +/- 2.50\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=-25.41 +/- 12.39\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=5400, episode_reward=20.18 +/- 1.46\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=6300, episode_reward=61.05 +/- 1.77\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=7200, episode_reward=86.04 +/- 2.24\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=8100, episode_reward=90.87 +/- 1.69\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=146.14 +/- 0.86\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9900, episode_reward=157.81 +/- 0.69\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=10800, episode_reward=126.76 +/- 1.11\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=11700, episode_reward=163.41 +/- 1.03\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=12600, episode_reward=162.99 +/- 0.67\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=13500, episode_reward=167.10 +/- 0.69\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=14400, episode_reward=158.59 +/- 1.15\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=15300, episode_reward=132.76 +/- 1.32\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=16200, episode_reward=104.73 +/- 1.51\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=17100, episode_reward=136.73 +/- 1.27\n",
      "Episode length: 180.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 17:48:43,187] Trial 14 finished with value: 186.3059869751334 and parameters: {'learning_rate': 0.0004956895877833771, 'gamma': 0.9932509044902064, 'batch_size': 64, 'buffer_size': 1000000, 'target_update_interval': 5000, 'net_arch': [128, 128]}. Best is trial 2 with value: 186.3059869751334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=18000, episode_reward=160.45 +/- 0.64\n",
      "Episode length: 180.00 +/- 0.00\n",
      "âœ… Training curve saved to: plots\\20250425_172910_trial14_rewards.png\n",
      "ðŸ“Š Total reward: 9446.17\n",
      "ðŸ“‰ Reward variation (std dev): 65.00\n",
      "Eval num_timesteps=900, episode_reward=-193.32 +/- 2.96\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=1800, episode_reward=-21.93 +/- 1.94\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=2700, episode_reward=-23.80 +/- 21.15\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=3600, episode_reward=-84.01 +/- 3.07\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=-12.12 +/- 1.76\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=5400, episode_reward=-1.07 +/- 1.62\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=6300, episode_reward=71.66 +/- 2.57\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=7200, episode_reward=41.82 +/- 4.58\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=8100, episode_reward=69.14 +/- 1.44\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-38.38 +/- 11.31\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=9900, episode_reward=77.41 +/- 4.00\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=10800, episode_reward=110.64 +/- 1.32\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=11700, episode_reward=115.60 +/- 1.05\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=12600, episode_reward=115.68 +/- 1.31\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=13500, episode_reward=80.32 +/- 1.50\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=14400, episode_reward=168.88 +/- 0.73\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=15300, episode_reward=161.64 +/- 0.91\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=16200, episode_reward=68.45 +/- 1.34\n",
      "Episode length: 180.00 +/- 0.00\n",
      "Eval num_timesteps=17100, episode_reward=-47.78 +/- 4.97\n",
      "Episode length: 180.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 17:49:55,209] Trial 15 finished with value: 186.3059869751334 and parameters: {'learning_rate': 0.00019076264179196379, 'gamma': 0.9893050542392028, 'batch_size': 32, 'buffer_size': 500000, 'target_update_interval': 5000, 'net_arch': [64, 64]}. Best is trial 2 with value: 186.3059869751334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=18000, episode_reward=-129.73 +/- 4.42\n",
      "Episode length: 180.00 +/- 0.00\n",
      "âœ… Training curve saved to: plots\\20250425_172910_trial15_rewards.png\n",
      "ðŸ“Š Total reward: 2862.38\n",
      "ðŸ“‰ Reward variation (std dev): 87.29\n",
      "Best value: 186.3059869751334\n",
      "Best params: {'learning_rate': 0.00011556060083557823, 'gamma': 0.9949461829904909, 'batch_size': 64, 'buffer_size': 1000000, 'target_update_interval': 5000, 'net_arch': [128, 128]}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import os\n",
    "from datetime import datetime\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from utils.plot_callback import PlotCallback\n",
    "\n",
    "PLOT_DIR = \"plots\"\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "RUN_ID   = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "eval_env = DiscretizedAquacultureEnv(region=\"north_sulawesi\")\n",
    "eval_cb  = EvalCallback(\n",
    "    eval_env,\n",
    "    n_eval_episodes=20,\n",
    "    eval_freq=180 * 5,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    lr     = trial.suggest_float(\"learning_rate\", 1e-4, 5e-4, log=True)\n",
    "    gamma  = trial.suggest_float(\"gamma\",           0.98,  0.995)\n",
    "    batch  = trial.suggest_categorical(\"batch_size\", [32, 64])\n",
    "    buf    = trial.suggest_categorical(\"buffer_size\", [500_000, 1000_000])\n",
    "    tgt_i  = trial.suggest_categorical(\"target_update_interval\", [5000, 10000])\n",
    "    net    = trial.suggest_categorical(\n",
    "                \"net_arch\",\n",
    "                [[64, 64], [128, 128]]\n",
    "             )\n",
    "    env = DiscretizedAquacultureEnv(region=\"north_sulawesi\")\n",
    "\n",
    "    model = DQN(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=lr,\n",
    "        gamma=gamma,\n",
    "        batch_size=batch,\n",
    "        buffer_size=buf,\n",
    "        target_update_interval=tgt_i,\n",
    "        exploration_final_eps=0.01,\n",
    "        policy_kwargs=dict(net_arch=net),\n",
    "        verbose=0,\n",
    "        tensorboard_log=\"./aqua_tensorboard\",\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "\n",
    "    plot_cb = PlotCallback(\n",
    "        window=1,\n",
    "        save_path=os.path.join(PLOT_DIR, f\"{RUN_ID}_trial{trial.number:02d}_rewards.png\"),\n",
    "        title=f\"DQN Trial {trial.number}\"\n",
    "    )\n",
    "\n",
    "    model.learn(180 * 100, callback=[eval_cb, plot_cb])\n",
    "    return eval_cb.best_mean_reward\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=16, timeout=3 * 3600)\n",
    "\n",
    "print(\"Best value:\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da244d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: False\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./aqua_tensorboard\\DQN_46\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 16.8     |\n",
      "|    exploration_rate | 0.868    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 555      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 720      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.202    |\n",
      "|    n_updates        | 154      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 13.9     |\n",
      "|    exploration_rate | 0.736    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 539      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 1440     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.275    |\n",
      "|    n_updates        | 334      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 9.24     |\n",
      "|    exploration_rate | 0.604    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 530      |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 2160     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.197    |\n",
      "|    n_updates        | 514      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 10.4     |\n",
      "|    exploration_rate | 0.472    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 527      |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 2880     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.161    |\n",
      "|    n_updates        | 694      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 24       |\n",
      "|    exploration_rate | 0.34     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 524      |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 3600     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0815   |\n",
      "|    n_updates        | 874      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 20       |\n",
      "|    exploration_rate | 0.208    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 520      |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 4320     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.259    |\n",
      "|    n_updates        | 1054     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 18.7     |\n",
      "|    exploration_rate | 0.076    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 517      |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 5040     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.806    |\n",
      "|    n_updates        | 1234     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 513      |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 5760     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.374    |\n",
      "|    n_updates        | 1414     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 6.65     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 509      |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 6480     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.465    |\n",
      "|    n_updates        | 1594     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 2.16     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 508      |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 7200     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.229    |\n",
      "|    n_updates        | 1774     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | -3.16    |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 508      |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 7920     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.243    |\n",
      "|    n_updates        | 1954     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | -7.91    |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 507      |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 8640     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.272    |\n",
      "|    n_updates        | 2134     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | -13.7    |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 506      |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 9360     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.417    |\n",
      "|    n_updates        | 2314     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | -18.5    |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 504      |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 10080    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.619    |\n",
      "|    n_updates        | 2494     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | -18.4    |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 503      |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 10800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.374    |\n",
      "|    n_updates        | 2674     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | -7.22    |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 504      |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 11520    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.306    |\n",
      "|    n_updates        | 2854     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | -1.88    |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 504      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 12240    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.304    |\n",
      "|    n_updates        | 3034     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 5.38     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 504      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 12960    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.242    |\n",
      "|    n_updates        | 3214     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 12.5     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 504      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 13680    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.151    |\n",
      "|    n_updates        | 3394     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 19.4     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 504      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 14400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.15     |\n",
      "|    n_updates        | 3574     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 25.6     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 504      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 15120    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.277    |\n",
      "|    n_updates        | 3754     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 30.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 504      |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 15840    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.23     |\n",
      "|    n_updates        | 3934     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 36.4     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 504      |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 16560    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.274    |\n",
      "|    n_updates        | 4114     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 41.4     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 504      |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 17280    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.333    |\n",
      "|    n_updates        | 4294     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 44.4     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 504      |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 18000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.1      |\n",
      "|    n_updates        | 4474     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 49.4     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 504      |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 18720    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.148    |\n",
      "|    n_updates        | 4654     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 55       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 504      |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 19440    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.124    |\n",
      "|    n_updates        | 4834     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 61.3     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 504      |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 20160    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.203    |\n",
      "|    n_updates        | 5014     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 65.3     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 503      |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 20880    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.275    |\n",
      "|    n_updates        | 5194     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 66.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 503      |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 21600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.228    |\n",
      "|    n_updates        | 5374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 71.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 502      |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 22320    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.167    |\n",
      "|    n_updates        | 5554     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 75.5     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 502      |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 23040    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.462    |\n",
      "|    n_updates        | 5734     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 81.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 502      |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 23760    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.197    |\n",
      "|    n_updates        | 5914     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 90.1     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 501      |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 24480    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.217    |\n",
      "|    n_updates        | 6094     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 96.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 501      |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 25200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.248    |\n",
      "|    n_updates        | 6274     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 103      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 501      |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 25920    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.197    |\n",
      "|    n_updates        | 6454     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 108      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 501      |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 26640    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.263    |\n",
      "|    n_updates        | 6634     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 115      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 501      |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 27360    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.104    |\n",
      "|    n_updates        | 6814     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 123      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 501      |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 28080    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.147    |\n",
      "|    n_updates        | 6994     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 130      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 501      |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 28800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0793   |\n",
      "|    n_updates        | 7174     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 128      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 501      |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 29520    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.243    |\n",
      "|    n_updates        | 7354     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 130      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 502      |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 30240    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.338    |\n",
      "|    n_updates        | 7534     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 129      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 502      |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 30960    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.227    |\n",
      "|    n_updates        | 7714     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 127      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 501      |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 31680    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.142    |\n",
      "|    n_updates        | 7894     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 126      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 501      |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 32400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.253    |\n",
      "|    n_updates        | 8074     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 124      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 501      |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 33120    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.35     |\n",
      "|    n_updates        | 8254     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 124      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 501      |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 33840    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.166    |\n",
      "|    n_updates        | 8434     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 122      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 501      |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 34560    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.145    |\n",
      "|    n_updates        | 8614     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 121      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 501      |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 35280    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.239    |\n",
      "|    n_updates        | 8794     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 120      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 501      |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 36000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.249    |\n",
      "|    n_updates        | 8974     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 120      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 501      |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 36720    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.345    |\n",
      "|    n_updates        | 9154     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 119      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 501      |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 37440    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.225    |\n",
      "|    n_updates        | 9334     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 117      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 501      |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 38160    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.304    |\n",
      "|    n_updates        | 9514     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 116      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 500      |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 38880    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.293    |\n",
      "|    n_updates        | 9694     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 117      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 501      |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 39600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.256    |\n",
      "|    n_updates        | 9874     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 118      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 501      |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 40320    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.369    |\n",
      "|    n_updates        | 10054    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 116      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 500      |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 41040    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.513    |\n",
      "|    n_updates        | 10234    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 112      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 500      |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 41760    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.312    |\n",
      "|    n_updates        | 10414    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 110      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 500      |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 42480    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.145    |\n",
      "|    n_updates        | 10594    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 110      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 500      |\n",
      "|    time_elapsed     | 86       |\n",
      "|    total_timesteps  | 43200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.154    |\n",
      "|    n_updates        | 10774    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 111      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 500      |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 43920    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.486    |\n",
      "|    n_updates        | 10954    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 112      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 500      |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 44640    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0968   |\n",
      "|    n_updates        | 11134    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 113      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 500      |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 45360    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.399    |\n",
      "|    n_updates        | 11314    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 109      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 500      |\n",
      "|    time_elapsed     | 92       |\n",
      "|    total_timesteps  | 46080    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.291    |\n",
      "|    n_updates        | 11494    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 107      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 500      |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 46800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.159    |\n",
      "|    n_updates        | 11674    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 107      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 499      |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 47520    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.346    |\n",
      "|    n_updates        | 11854    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 106      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 499      |\n",
      "|    time_elapsed     | 96       |\n",
      "|    total_timesteps  | 48240    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.33     |\n",
      "|    n_updates        | 12034    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 107      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 499      |\n",
      "|    time_elapsed     | 97       |\n",
      "|    total_timesteps  | 48960    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.214    |\n",
      "|    n_updates        | 12214    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 107      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 499      |\n",
      "|    time_elapsed     | 99       |\n",
      "|    total_timesteps  | 49680    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.353    |\n",
      "|    n_updates        | 12394    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 107      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 499      |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 50400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.415    |\n",
      "|    n_updates        | 12574    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 105      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 499      |\n",
      "|    time_elapsed     | 102      |\n",
      "|    total_timesteps  | 51120    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.112    |\n",
      "|    n_updates        | 12754    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 105      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 499      |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total_timesteps  | 51840    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.195    |\n",
      "|    n_updates        | 12934    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 102      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 499      |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 52560    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.379    |\n",
      "|    n_updates        | 13114    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 100      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 499      |\n",
      "|    time_elapsed     | 106      |\n",
      "|    total_timesteps  | 53280    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.24     |\n",
      "|    n_updates        | 13294    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 180      |\n",
      "|    ep_rew_mean      | 101      |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 499      |\n",
      "|    time_elapsed     | 108      |\n",
      "|    total_timesteps  | 54000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.304    |\n",
      "|    n_updates        | 13474    |\n",
      "----------------------------------\n",
      "âœ… Training curve saved to: plots/dqn_training_rewards.png\n",
      "ðŸ“Š Total reward: 26578.93\n",
      "ðŸ“‰ Reward variation (std dev): 70.88\n",
      "ðŸ’¾ Model saved to ./saved_model/dqn_best_model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from utils.plot_callback import PlotCallback\n",
    "\n",
    "# Check CUDA and env\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "env = DiscretizedAquacultureEnv(region=\"north_sulawesi\")\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# Best hyperparameters from tuning\n",
    "best_params = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"gamma\": 0.995,\n",
    "    \"batch_size\": 64,\n",
    "    \"buffer_size\": 1000000,\n",
    "    \"target_update_interval\": 5000,\n",
    "    \"net_arch\": [128, 128],\n",
    "    \"exploration_final_eps\": 0.01\n",
    "}\n",
    "\n",
    "# Instantiate DQN with fixed final epsilon\n",
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    gamma=best_params[\"gamma\"],\n",
    "    batch_size=best_params[\"batch_size\"],\n",
    "    buffer_size=best_params[\"buffer_size\"],\n",
    "    target_update_interval=best_params[\"target_update_interval\"],\n",
    "    exploration_final_eps=best_params[\"exploration_final_eps\"],\n",
    "    policy_kwargs=dict(net_arch=best_params[\"net_arch\"]),\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./aqua_tensorboard\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Training callback\n",
    "plot_cb = PlotCallback(\n",
    "    window=1,\n",
    "    save_path=\"plots/dqn_training_rewards.png\",\n",
    "    title=f\"DQN Training Rewards (Îµ={best_params['exploration_final_eps']})\"\n",
    ")\n",
    "\n",
    "# Train\n",
    "model.learn(total_timesteps=180 * 300, callback=plot_cb)\n",
    "\n",
    "# Save\n",
    "model_save_path = \"./saved_model/dqn_best_model\"\n",
    "os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "model.save(model_save_path)\n",
    "print(f\"ðŸ’¾ Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b927722",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AquacultureEnv.__init__() got an unexpected keyword argument 'discrete'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplot_callback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PlotCallback\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Initialize the environment\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m env = \u001b[43mAquacultureEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiscrete\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Load the trained DQN model\u001b[39;00m\n\u001b[32m     17\u001b[39m model = DQN.load(\u001b[33m\"\u001b[39m\u001b[33mmodels/dqn_aquaculture\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# Update path as needed\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: AquacultureEnv.__init__() got an unexpected keyword argument 'discrete'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from utils.calculation import Calculation\n",
    "\n",
    "obs, _ = env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "total_reward = 0\n",
    "\n",
    "print(\"Discrete action space size:\", env.action_space.n)\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    action_idx, _ = model.predict(obs, deterministic=True)\n",
    "    action = env.discrete_actions[action_idx]\n",
    "\n",
    "    raw = env.base_env.denormalize(obs)\n",
    "    biomass, fish_count, temp, do_level, uia = raw\n",
    "    feed_amount = Calculation.compute_feed_weight(action[0], biomass)\n",
    "    print(f\"\"\"\\n--- Day {env.base_env.day + 1} ---\n",
    "Raw-obs: biomass={biomass:.1f} g, count={fish_count:.0f}, temp={temp:.2f}Â°C, DO={do_level:.2f} mg/L, UIA={uia:.3f} mg/L\n",
    "Action: feed_rate={action[0]:.3f} â†’ feed_amt={feed_amount:.2f} g, temp_set={action[1]:.3f}, aeration_rate={action[2]:.2f}mg/L\n",
    "\"\"\")\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env.step(action_idx)\n",
    "    total_reward += reward\n",
    "\n",
    "    print(f\"\"\"Reward Breakdown:\n",
    "Fish Value Gain:     {info['fish_value']} $\n",
    "Feed Cost:           {info['feed_cost']:.2f} $\n",
    "Heat Cost:           {info['heat_cost']:.2f} $\n",
    "Oxygenation Cost:    {info['oxygenation_cost']:.2f} $\n",
    "â†’ Net Reward:        {info['reward']:.2f} $\n",
    "\"\"\")\n",
    "\n",
    "    env.render()\n",
    "\n",
    "    if hasattr(env.base_env, 'exit_requested') and env.base_env.exit_requested:\n",
    "        break\n",
    "\n",
    "    time.sleep(0.01)\n",
    "\n",
    "env.close()\n",
    "print(\"Total Reward:\", total_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
