# ğŸŒŠ AQUA-GYM: Reinforcement Learning for Smart Aquaculture Management

Optimize fish feeding, energy use, and water quality using Reinforcement Learning (RL) to enhance sustainability and profitability in inland tank aquaculture.

<video src="./demo.mp4" controls width="600">
  Your browser does not support the video tag.
</video>

---

## ğŸ§  Core Concepts

### âœ… Objective

1. Maximize **profitability** via feed optimization.
2. Reduce **feed wastage** and overfeeding.
3. Enhance **water quality** (DO, UIA, Temp).
4. Improve **energy efficiency** in aeration/heating.

### ğŸ¯ Reinforcement Learning Setup

| Component       | Description                                                            |
| --------------- | ---------------------------------------------------------------------- |
| **State (S)**   | Biomass, Fish Count, Temperature, DO, UIA                              |
| **Action (A)**  | Feeding Rate, Target Temp, Aeration Rate                               |
| **Reward (R)**  | Fish value gain â€“ Feed cost â€“ Energy cost                              |
| **Agent (Ï€)**   | Learns feeding & control policy from real-time feedback                |
| **Environment** | OpenAI Gym-compatible, built using `aquaculture_env.py` and DEB theory |

---

## ğŸ¤– RL Algorithms Used

| Algorithm  | Type                              | Description                                            |
| ---------- | --------------------------------- | ------------------------------------------------------ |
| **TD3**    | Model-Free                        | Best for continuous control like feeding rate and temp |
| **SAC**    | Model-Free                        | Entropy-based for better exploration                   |
| **DQN**    | Model-Free                        | Discrete actions, less efficient for continuous tasks  |
| **Dyna-Q** | Hybrid (Model-Based + Model-Free) | Combines simulation & real data learning               |

---

## ğŸ“ˆ Evaluation Metrics

| Metric                          | Purpose                             |
| ------------------------------- | ----------------------------------- |
| **Total Reward**                | Overall performance indicator       |
| **Feed Conversion Ratio (FCR)** | Feed efficiency (lower = better)    |
| **Specific Growth Rate (SGR)**  | Growth performance                  |
| **Energy Efficiency**           | Value gain per energy cost          |
| **Profit Margin**               | Profitability of the farm operation |

---

## ğŸŒ Deployment Environments

The RL agents were tested under 3 real-world region profiles:

| Region            | Challenge Level | Differences                 |
| ----------------- | --------------- | --------------------------- |
| Guangdong ğŸ‡¨ğŸ‡³      | Hard            | High energy cost            |
| North Sulawesi ğŸ‡®ğŸ‡© | Medium          | Balanced conditions         |
| Kafr El-Shaikh ğŸ‡ªğŸ‡¬ | Easy            | Cheap energy, low feed cost |

---

## ğŸ“Š Key Findings

- **SAC** outperformed others across all KPIs (avg. 90.6%)
- **Dyna-Q** performed well in harder scenarios with less training data
- **TD3** provided stable but moderate performance
- **DQN** was less suitable due to discrete action limitations

---

## ğŸ“ Project Structure

```text
aqua-gym/
â”œâ”€â”€ agent/              # Reinforcement learning agents (e.g., Dyna-Q)
â”œâ”€â”€ assets/             # Fish images and tank sound effects
â”œâ”€â”€ envs/               # Aquaculture simulation environments (Gym-style)
â”œâ”€â”€ model/              # Fish growth and environment models
â”œâ”€â”€ plots/              # Hyperparameter tuning & exploration experiments
â”œâ”€â”€ saved_model/        # Trained RL models parameters
â”œâ”€â”€ utils/              # Configs, metric calculations, plotting tools
â”œâ”€â”€ parameters.yaml     # Environment configuration
â”œâ”€â”€ FINAL_EVALUATION.ipynb  # Model comparison notebook
â”œâ”€â”€ test_*.ipynb        # Testing and evaluation notebooks
â”œâ”€â”€ README.md           # Project documentation

```
